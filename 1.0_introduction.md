### 上帝视角
对于一个没有任何计算机背景的门外汉，如何解释**机器学习**这个概念呢？
这个问题其实一点都不简单，因为能把别人认为复杂的问题说简单了也是一种本领。

后来我就想到一种理解的思路：不管再牛的概念也好，技术也好，终归是为了解决问题的，那就学**Polya**看看是否可以从如何解题的角度将**机器学习**讲清楚。 

这里先假设大家都有一些数学基础，比如了解**函数**的概念，**函数**简单讲就是一种映射，输入是变量(x)，输出是我们想要的结果(y)，使用函数的时候就是给它一个x，它就会返回对应的y。也就是我们通过**函数**是想解决这样一类问题：**将已知的信息输给函数，期望得到我们想要的答案**。

如果讲到这里你是理解的，那么我可以明确的告诉你：机器学习的目的就是训练一个函数(机器学习里叫模型)，然后将已知的信息输给它，期望它返回我们想要的答案，比如用户是否会点击你推荐的某条新闻，典型的二分类问题！

    注：当然上面这个简单的理解一定不是精准、不完备的，这里假定我们讲的机器学习主要是监督学习。

### 形式化表达

下面我们换种更容易理解的方式，进一步探讨机器学习。相比于文字，大家更容易理解图形，因为图形的表达是形式化的，非常直观。

下面我们就从一个简单的例子开始说起：假定我们已知的信息可以抽象为多维空间中的一系列点，点分为正和负两种，我们的目的是希望可以找到一个超平面，可以将这些点能够很好地将这些点区分开，如下图所示：

![正负样本分类问题](https://upload-images.jianshu.io/upload_images/13018728-e110cec9c78c325e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假定上图中的实心点为正样本，空心点为负样本，目的就是找到一条直线可以将这两类样本很好的分开，图中给出了三条直线(H1、H2、H3)，仅靠直觉就可以很容易得出结论：H2和H3都可以将样本点分开，但是H3分的效果更好，H1不满足我们的要求。

这里引入几个通用概念：图中已知的点就是我们的**样本点**，直线就是我们求解的**决策边界**，而对于这种二分类问题，这里的**决策边界**就是**模型**的概念。

很容易发现上图中的样本点正负已知，机器学习的目的不可能是求解已知问题，上图中找到最优决策边界的过程实际是模型训练的过程，模型训练好后真正发挥作用的是用它来预测未知点的正负标签。因此上面提到的**样本点**集合其实是**训练样本**。

这里可以先总结下当前可以认知的**机器学习**概念：**根据已知训练样本，学习一条决策边界，即模型，用来预测未知样本的正负标签。**

### 核心问题

首先，重温上节的例子，机器学习首先是要学习到一条决策边界，假定我们的要求就是可能将正负样本完全正确的分开，那么决策边界H2和H3都是满足要求的，答案不唯一，那么**模型**就可以理解为是由很多满足要求的决策边界组成的解的结合，又叫做**假设空间**。

但是我们只能选择一条决策边界，而且很明显H3是优于H2的，那么第一个核心问题就是：如何量化我们的直觉，从而在**假设空间**中找到最优解。

咱们可以看图脑爆，很明显H2距离边界上的样本点的距离更近，如果你想到了这点，那么恭喜你，你发现了支持向量，量化这个概念就可以得到大名鼎鼎的支持向量机模型了；继续脑爆，你可能会想可不可以用所有点到决策边界的距离和的大小来作为选择最优解的评价指标，想到这里的人，真的越来越接近机器学习的真谛了，因为这里不仅提到了利用**平方损失**求解方法，还提到了一个很核心的概念，叫**评价指标**，在机器学习中，这个概念通常被称为**目标函数**或**损失函数**。这个概念真的很重要、很重要、很重要！！！

介于这个概念如此重要，我再啰嗦两句：**损失函数**很显然是越小越好，因为损失越小代表了精度越高，这个很容易理解；而**目标函数**就未必是越小越好，也可以是越大越好，因此这两个概念并不是完全等价的，当**目标函数**越小越好时，这两个概念是等价的；而当**目标函数**越大越好时(比如最大似然估计)，这两个概念是相反的关系。

本节我们所讲的内容其实是机器学习三大核心概念之二：**策略**，**策略**就是在模型的**假设空间**中为了找到最优解的方法。上面我们提到：先定义**损失函数**，然后最小化损失函数求解最优解，这是机器学习中非常常见的策略。而机器学习三大核心概念之一其实就是前面提到的**模型**，另一个核心概念就是**算法**，很快会讲到。

### 满脑子疑问

